{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"I love running and playing football! #sports\",)]\n",
    "df = spark.createDataFrame(data, [\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming (using Porter's algorithm)\n",
    "def stem(word):\n",
    "    suffixes = ['s', 'es', 'ed', 'ing', 'ly']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize(word):\n",
    "    # Add your own lemmatization rules here\n",
    "    lemmatized_word = word  # Placeholder, implement your logic\n",
    "    return lemmatized_word\n",
    "\n",
    "# Regex removal\n",
    "def remove_regex(text):\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "# Hashtag removal\n",
    "def remove_hashtags(text):\n",
    "    pattern = r\"#\\w+\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "# Stopwords removal\n",
    "def remove_stopwords(text):\n",
    "    # Hard-coded stopwords\n",
    "    stopwords = ['the', 'and', 'a', 'is', 'in']  # Add your own list of stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs (User-Defined Functions)\n",
    "stem_udf = spark.udf.register(\"stem\", stem)\n",
    "lemmatize_udf = spark.udf.register(\"lemmatize\", lemmatize)\n",
    "remove_regex_udf = spark.udf.register(\"remove_regex\", remove_regex)\n",
    "remove_hashtags_udf = spark.udf.register(\"remove_hashtags\", remove_hashtags)\n",
    "remove_stopwords_udf = spark.udf.register(\"remove_stopwords\", remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------------------------------+--------------------------------------------+------------------------------------------+-------------------------------------+\n",
      "|text                                        |stemmed_text                               |lemmatized_text                             |regex_removed_text                        |hashtags_removed_text                |\n",
      "+--------------------------------------------+-------------------------------------------+--------------------------------------------+------------------------------------------+-------------------------------------+\n",
      "|I love running and playing football! #sports|I love running and playing football! #sport|I love running and playing football! #sports|I love running and playing football sports|I love running and playing football! |\n",
      "+--------------------------------------------+-------------------------------------------+--------------------------------------------+------------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "df = df.withColumn(\"stemmed_text\", stem_udf(\"text\"))\n",
    "df = df.withColumn(\"lemmatized_text\", lemmatize_udf(\"text\"))\n",
    "df = df.withColumn(\"regex_removed_text\", remove_regex_udf(\"text\"))\n",
    "df = df.withColumn(\"hashtags_removed_text\", remove_hashtags_udf(\"text\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+\n",
      "|                text|      label|      Date|\n",
      "+--------------------+-----------+----------+\n",
      "|Ex Wife Threateni...|    suicide|2022-05-10|\n",
      "|Am I weird I don'...|non-suicide|2022-05-10|\n",
      "|Finally 2020 is a...|non-suicide|2022-05-10|\n",
      "|i need helpjust h...|    suicide|2022-05-10|\n",
      "|Iâm so lostHell...|    suicide|2022-05-10|\n",
      "|Honetly idkI dont...|    suicide|2022-05-10|\n",
      "|[Trigger warning]...|    suicide|2022-05-10|\n",
      "|It ends tonight.I...|    suicide|2022-05-10|\n",
      "|Everyone wants to...|non-suicide|2022-05-10|\n",
      "|My life is over a...|    suicide|2022-05-10|\n",
      "|I took the rest o...|    suicide|2022-05-10|\n",
      "|Can you imagine g...|    suicide|2022-05-10|\n",
      "|Do you think gett...|    suicide|2022-05-10|\n",
      "|death, continuedI...|    suicide|2022-05-11|\n",
      "|Been arrested - f...|    suicide|2022-05-11|\n",
      "|Fuck the verizon ...|non-suicide|2022-05-11|\n",
      "|Iâm scared.   E...|    suicide|2022-05-11|\n",
      "|Well, Im screwed....|non-suicide|2022-05-11|\n",
      "|I'm fucked assign...|non-suicide|2022-05-11|\n",
      "|yeaputting a knif...|    suicide|2022-05-11|\n",
      "+--------------------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "pandas_df = pd.read_csv('SuicideData.csv')\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "# Show the DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import re\n",
    "df = df.withColumn(\"stemmed_text\", stem_udf(\"text\"))\n",
    "df = df.withColumn(\"lemmatized_text\", lemmatize_udf(\"text\"))\n",
    "df = df.withColumn(\"regex_removed_text\", remove_regex_udf(\"text\"))\n",
    "df = df.withColumn(\"hashtags_removed_text\", remove_hashtags_udf(\"text\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDBIntegration\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://localhost:27017/Twitter.SuicideData\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/Twitter.SuicideData\") \\\n",
    "    .getOrCreate()\n",
    "df.write.format(\"mongo\").mode(\"append\").save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "-1.-1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
